{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/hcaoaf/.local/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.neural_network import *\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train,test = pd.read_csv('train.csv'),pd.read_csv('test.csv')\n",
    "data = pd.concat([train,test], ignore_index=True)\n",
    "#some features are masked by 0, recover\n",
    "data.loc[data.age==0, 'age'] = None\n",
    "data.loc[data.charge_sensitivity==0, 'charge_sensitivity'] = data.charge_sensitivity.median()\n",
    "data.internet_age = data.internet_age.apply(lambda x: x/12)\n",
    "\n",
    "trn_data, test_data = data[:len(train)],data[len(train):]\n",
    "trn_data.drop(['id','age',], axis=1, inplace=True)\n",
    "del test_data['credit']\n",
    "target = trn_data['credit']\n",
    "del trn_data['credit']\n",
    "\n",
    "X_trn, X_val, y_trn, y_val = train_test_split(trn_data, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.neural_network import *\n",
    "\n",
    "def base_model(regressor, X_trn, X_val, y_trn, y_val,K_fold=5,Scale=False):\n",
    "    X_trn, y_trn = X_trn.to_numpy(), y_trn.to_numpy()\n",
    "    X_val, y_val = X_val.to_numpy(), y_val.to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    if Scale:\n",
    "        pipe = Pipeline([('PreTransformer', scaler),\\\n",
    "            ('Regressor', regressor)])\n",
    "    else:\n",
    "        pipe = Pipeline([('Regressor', regressor)])\n",
    "    print(pipe)\n",
    "\n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    folds = KFold(n_splits=K_fold,shuffle=True,random_state=0)\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_trn, y_trn)):\n",
    "        print(f\"\\n======fold No.{fold_+1}======\")\n",
    "        print('CV train_sample size : %d' % len(trn_idx))\n",
    "        trn_X, trn_y = X_trn[trn_idx], y_trn[trn_idx]\n",
    "        val_X, val_y = X_trn[val_idx], y_trn[val_idx]\n",
    "        #fit\n",
    "        pipe.fit(trn_X, trn_y)\n",
    "        #predict on k_fold splitted val_X\n",
    "        val_y_pred = pipe.predict(val_X)\n",
    "        print('CV MAE Loss: %.2f' % mean_absolute_error(val_y_pred, val_y))\n",
    "        #predict on X_val\n",
    "        y_pred += pipe.predict(X_val)\n",
    "    #div K-fold number\n",
    "    y_pred /= K_fold\n",
    "    #final metric on val-set\n",
    "    MAE = mean_absolute_error(y_pred, y_val)\n",
    "    print('\\nMAE Loss: %.5f' % MAE)\n",
    "    print('Final Score: %.5f' % (1/(1+MAE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(X_trn, X_val, y_trn, y_val,K_fold=5):\n",
    "    X_trn, y_trn = X_trn.to_numpy(), y_trn.to_numpy()\n",
    "    X_val, y_val = X_val.to_numpy(), y_val.to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_trn)\n",
    "    X_trn = scaler.transform(X_trn)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    folds = KFold(n_splits=K_fold,shuffle=True,random_state=0)\n",
    "\n",
    "    reg = lgb.LGBMRegressor(objective='regression',reg_alpha=0.6, n_estimators=10000)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_trn, y_trn)):\n",
    "        print(f\"\\n======fold No.{fold_+1}======\")\n",
    "        print('CV train_sample size : %d' % len(trn_idx))\n",
    "        trn_X, trn_y = X_trn[trn_idx], y_trn[trn_idx]\n",
    "        val_X, val_y = X_trn[val_idx], y_trn[val_idx]\n",
    "        #fit\n",
    "        reg.fit(trn_X, trn_y, eval_set = [(val_X,val_y)],early_stopping_rounds=500,verbose=10)\n",
    "        #predict on k_fold splitted val_X\n",
    "        val_y_pred = reg.predict(val_X, num_iteration = reg.best_iteration_)\n",
    "        print('CV MAE Loss: %.2f' % mean_absolute_error(val_y_pred, val_y))\n",
    "        #predict on X_val\n",
    "        y_pred += reg.predict(X_val, num_iteration = reg.best_iteration_) / K_fold\n",
    "    #final metric on val-set\n",
    "    MAE = mean_absolute_error(y_pred, y_val)\n",
    "    print('\\nMAE Loss: %.5f' % MAE)\n",
    "    print('Final Score: %.5f' % (1/(1+MAE)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_pred = lgb_model1(X_trn, X_val, y_trn, y_val,K_fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params_v1 = {'depth': 6, 'learning_rate': 0.8, 'l2_leaf_reg': 2, 'num_boost_round': 10000, 'random_seed': 94,\n",
    "                 'loss_function': 'MAE'}\n",
    "def cat_model(X_trn, X_val, y_trn, y_val, K_fold=10, cat_params=cat_params_v1):\n",
    "    X_trn, y_trn = X_trn.to_numpy(), y_trn.to_numpy()\n",
    "    X_val, y_val = X_val.to_numpy(), y_val.to_numpy()\n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    folds = KFold(n_splits=K_fold, shuffle=True, random_state=0)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_trn, y_trn)):\n",
    "        print(f\"\\n======fold No.{fold_+1}======\")\n",
    "        print('CV train_sample size : %d' % len(trn_idx))\n",
    "        regor = cat.CatBoostRegressor(**cat_params)\n",
    "\n",
    "        regor.fit(X_trn[trn_idx], y_trn[trn_idx], early_stopping_rounds=200, verbose_eval=1000,\n",
    "                  use_best_model=True, eval_set=(X_trn[val_idx], y_trn[val_idx]))\n",
    "        y_pred += regor.predict(X_val) / K_fold\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pred = cat_model(X_trn, X_val, y_trn, y_val, K_fold=10, cat_params=cat_params_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_sklearn(X_trn, X_val, y_trn, y_val, K_fold=5):\n",
    "    X_trn, y_trn = X_trn.to_numpy(), y_trn.to_numpy()\n",
    "    X_val, y_val = X_val.to_numpy(), y_val.to_numpy()\n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    folds = KFold(n_splits=K_fold, shuffle=True, random_state=0)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_trn, y_trn)):\n",
    "        print(f\"\\n======fold No.{fold_+1}======\")\n",
    "        print('CV train_sample size : %d' % len(trn_idx))\n",
    "        trn_X, trn_y = X_trn[trn_idx], y_trn[trn_idx]\n",
    "        val_X, val_y = X_trn[val_idx], y_trn[val_idx]\n",
    "        regor = xgb.XGBRegressor(n_estimators=1000, max_depth=6, learning_rate=0.004, subsample=0.5,\n",
    "                                 colsample_bytree=0.5, reg_alpha=0.2, n_jobs=-1, verbosity=0)\n",
    "\n",
    "        regor.fit(trn_X, trn_y, eval_metric='mae', eval_set = [(val_X,val_y)],\n",
    "                  early_stopping_rounds=200, verbose=True)\n",
    "        y_pred += regor.predict(X_val,\n",
    "                                ntree_limit=regor.best_ntree_limit) / K_fold\n",
    "\n",
    "    # final metric on val-set\n",
    "    MAE = mean_absolute_error(y_pred, y_val)\n",
    "    print('\\nMAE Loss: %.5f' % MAE)\n",
    "    print('Final Score: %.5f' % (1/(1+MAE)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n======fold No.1======\nCV train_sample size : 32000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "XGBoostError",
     "evalue": "value 10 for Parameter verbosity exceed bound [0,3]\nverbosity: Flag to print out detailed breakdown of runtime.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-42cd597cf7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e9ebf74ddf5c>\u001b[0m in \u001b[0;36mxgb_sklearn\u001b[0;34m(X_trn, X_val, y_trn, y_val, K_fold)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         regor.fit(trn_X, trn_y, eval_metric='mae', eval_set = [(val_X,val_y)],\n\u001b[0;32m---> 16\u001b[0;31m                   early_stopping_rounds=200, verbose=True)\n\u001b[0m\u001b[1;32m     17\u001b[0m         y_pred += regor.predict(X_val,\n\u001b[1;32m     18\u001b[0m                                 ntree_limit=regor.best_ntree_limit) / K_fold\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         )\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     74\u001b[0m             show_stdv=False, cvfolds=None)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/callback.py\u001b[0m in \u001b[0;36mbefore_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;34m'''Function called before training.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'before_training should return the model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/callback.py\u001b[0m in \u001b[0;36mbefore_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbefore_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarting_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_boosted_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mnum_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2000\u001b[0m         \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterBoostedRounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: value 10 for Parameter verbosity exceed bound [0,3]\nverbosity: Flag to print out detailed breakdown of runtime."
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb_sklearn(X_trn, X_val, y_trn, y_val, K_fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error((lgb_pred + cat_pred + xgb_pred)/3, y_val)"
   ]
  },
  {
   "source": [
    "def ensemble(X_val,y_val,models):\n",
    "    X_val, y_val = X_val.to_numpy(), y_val.to_numpy()\n",
    "    y_pred = np.zeros_like(y_val)\n",
    "    for model in models:\n",
    "        y_pred += model.predict(X_val,\n",
    "                                num_iteration = model.best_iteration_) / len(models)\n",
    "    # final metric on val-set\n",
    "    MAE = mean_absolute_error(y_pred, y_val)\n",
    "    print('\\nMAE Loss: %.5f' % MAE)\n",
    "    print('Final Score: %.5f' % (1/(1+MAE)))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}